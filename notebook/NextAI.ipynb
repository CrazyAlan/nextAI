{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final goal is to build intelligent robot that can play with people. We want to use the cutting-edge deep learning\n",
    "that help the robot to see, understand and react. Even though many deep learning methods are open sourced online, we need\n",
    "to build an integrated system. And we need to find out the balance between accuracy and speed, since everyone wants an agile \n",
    "robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our idea of applying deep learning methods to robot, in this assignment we choose to apply face recognition onto robot. Another consideration is in order to let the robot inteact with people, the first step is to help the robot recognize people. Even though face recognition is an active research topic and there are many open-sourced project for face recognition, for our own robot we want it to be accurate and fast. It's not just simply calling some api, we want to integate it into our whole system and know how to optimize it.\n",
    "\n",
    "We start from tranning our model for face recognition from a dataset containing 10,575 people and 494,414 images. Different networks architectures have been tested. And finally, we integrate the detecting, tracking and recognizing for real-time performance.\n",
    "\n",
    "We build the whole project based on David's [5] tensorflow implementation of facenet [1], and referenced from the CMU project OpenFace[6] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the results for the whole system:\n",
    "    1. Face recognization accuracy: \n",
    "        a. Google Inception Net V1: 99% on LFW datset\n",
    "        b. MFM network: 93% on LFW dataset\n",
    "    2. Face classification accuracy: ~100% with a datsets with 10 people\n",
    "    2. Face recognization time usage on cpu: 50~100ms/One person, 100~200ms/Two person, ?/Three person\n",
    "    3. Face recognization time usage on gpu: roughly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from using David's[5] pre-trained model using Inception-V1, trainned on Casia dataset [7]. This model has ~99% accuracy on LFW dataset, which means it's already good enough for us to use!! But considering the time efficiency and memory usage for such a large network, we have to consider some small network that can improve the speed. Actually since the robot usally need to classify within 10 people, it doesn't need a pretty strong model, but it needs efficiency.\n",
    " \n",
    "The work by Wu etc. [3] introdcued a lightened network for face recognition, and the performance a still acceptable. So in this project, we chose to implement that network and use it in our final demo system. The network architecture is as following   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning is always not easy for face recognition, and choosing loss function is highly related to final face representation. Facenet [1] used the tripple loss, which tries to maximize the distance between different people but minimize the distance between the same person. But during the trainning process, you need to do hard-negative example mining in order to increase the performance. Thus it always take a long time and the convergence is poor.\n",
    "\n",
    "Parkhi [8] found out that actually in stead of using triple loss at first, using classifier loss to pre-train the network will help the network to converge well. Wen [2] recently proposed one neat idea of using center loss. Actually the problem of using classifier loss is the variation if large within one class, since the classifier is just tring to find out a shape that can accuratly classify different classes. For the center loss, it added the variational penalty into the loss, so it forced the variance to be small for the same person. In our implementation we choosed to train classifier and using the center loss. \n",
    "\n",
    "In order to train face recognition, we first need to crop out the thumbnail of the face. We first use the Zhang's [8] method to detect the face, this method tries to detect people's face through 3 small network and the performance is pertty good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's be able to include the large dataset here, I simply write down the files that we used to train the model\n",
    "1. Cropping the thumbnail: align_dataset_mtcnn.py\n",
    "2. Tranning Classifier: facenet_train_classifier.py\n",
    "3. Lightened, Inception_V1 network: lightened.py, inception_resnet_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract People's Feature Using Trainned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the face recognition model, we can use the model to extract the feature for each person and then train a svm classifier. We put all the needed data inside the \"Content\" folder. In order to extract feature for new person, simply place new photos under the \"People\" folder and name the new folder with the person's name. The program will automatically crop the photo and use the trained neural network to extract the feature for all people  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solving PYTHONPATH Problem\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '/home/xca64/remote/GitHub/facenet/src')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a long time, cause it will crop the feature at first and then extracte the features for each person. And make sure the PYTHONPATH is set to the src folder as previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../src/align/train_person_recognition_with_dlib.py --input_dir ../Content/People --output_dir ../Content/People_Cropped --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --feature_dir ../Content/PeopleFeature --feature_name emb_array \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The extracted feature are stored in the PeopleFeature dir, and you can have a look of the feature by\n",
    "# uncommenting followed code\n",
    "\n",
    "# npfile=np.load('../Content/PeopleFeature/emb_array.npz')\n",
    "# print('People Feature Example ', npfile['emb_array'][0])\n",
    "# print('People Label', npfile['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.972222222222\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.972222222222\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, at first we tested the system's accuracy to ten-fold cross validation by 25 times\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1 --test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning and Storing SVM model\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, and store the trainned SVM model\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Model directory: ../Content/NetworkModel/20170321-222346\n",
      "Loading network used to extract features\n",
      "Metagraph file: model-20170321-222346.meta\n",
      "Checkpoint file: model-20170321-222346.ckpt-80000\n",
      "../Content/Test/photos/Xiaochuan/0.jpg\n",
      "../Content/Test/photos/Xiaochuan/1.jpg\n",
      "../Content/Test/photos/Xiaochuan/2.jpg\n",
      "../Content/Test/photos/Xiaochuan/3.jpg\n",
      "../Content/Test/photos/Xiaochuan/4.jpg\n",
      "../Content/Test/photos/Xiaochuan/5.jpg\n",
      "../Content/Test/photos/Xiaochuan/6.jpg\n",
      "../Content/Test/photos/Xiaochuan/7.jpg\n",
      "Total number of images: 8\n",
      "Number of successfully aligned images: 8\n",
      "\n",
      "\n",
      "Starting to extract features\n",
      "Crop pics spend 0.461 seconds\n"
     ]
    }
   ],
   "source": [
    "# The test photo are located in the test folder and the predicted label should be me\n",
    "# This step may take a long time, cause it will load the tensorflow model\n",
    "# It may crash the notebook, seems like the dlib problem, but you can copy the commands to terminal and run it\n",
    "%run ../src/align/recognize_person_with_dlib.py --input_dir ../Content/Test/photos --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Florian Schroff, Dmitry Kalenichenko, James Philbin; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 815-823\n",
    "\n",
    "[2] Wen, Yandong, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. \"A discriminative feature learning approach for deep face recognition.\" In European Conference on Computer Vision, pp. 499-515. Springer International Publishing, 2016.\n",
    "\n",
    "[3] Wu, Xiang, Ran He, and Zhenan Sun. \"A lightened cnn for deep face representation.\" In 2015 IEEE Conference on IEEE Computer Vision and Pattern Recognition (CVPR). 2015.\n",
    "\n",
    "[4] Zhang, Kaipeng, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. \"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\" IEEE Signal Processing Letters 23, no. 10 (2016): 1499-1503.\n",
    "\n",
    "[5] https://github.com/davidsandberg/facenet\n",
    "\n",
    "[6] https://cmusatyalab.github.io/openface/\n",
    "\n",
    "[7] Dong Yi, Zhen Lei, Shengcai Liao and Stan Z. Li, “Learning Face Representation from Scratch”. arXiv preprint[] arXiv:1411.7923. 2014\n",
    "\n",
    "[8] Parkhi, Omkar M., Andrea Vedaldi, and Andrew Zisserman. \"Deep Face Recognition.\" In BMVC, vol. 1, no. 3, p. 6. 2015.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
