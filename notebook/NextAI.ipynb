{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final goal is to build intelligent robot that can play with people. We want to use the cutting-edge deep learning\n",
    "methods that help the robot to see, understand and react. Even though many deep learning methods are open sourced online, we need\n",
    "to build an integrated system. And we need to find out the balance between accuracy and speed, since everyone wants an agile \n",
    "robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our idea of applying deep learning methods to robot, in this assignment we choose to apply face recognition onto robot. Another consideration is in order to let the robot inteact with people, the first step is to help the robot recognize people. Even though face recognition is an active research topic and there are many open-sourced project for face recognition, for our own robot we want it to be accurate and fast. It's not just simply calling some api, we want to integate it into our whole system and know how to optimize it.\n",
    "\n",
    "We start from tranning our model for face recognition from a dataset containing 10,575 people and 494,414 images. Different networks architectures have been tested. And finally, we integrated the detecting, tracking and recognizing for real-time performance.\n",
    "\n",
    "We build the whole project based on David's [5] tensorflow implementation of facenet [1], and referenced from the CMU project OpenFace[6] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the results for the whole system:\n",
    "    1. Face recognization accuracy: \n",
    "        a. Google Inception Net V1: 99% on LFW datset\n",
    "        b. Lightened network: 93% on LFW dataset\n",
    "    2. Face classification accuracy: ~100% with a datsets with 10 people\n",
    "    2. Face recognization time usage on cpu: 50~100ms/One person, 100~150ms/Two person, 150~200ms/Three person\n",
    "    3. Face recognization time usage on gpu: roughly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from using David's[5] pre-trained model based on Inception-V1, trainned on Casia dataset [7]. This model has ~99% accuracy on LFW dataset, which means it's already good enough for us to use!! But considering the time efficiency and memory usage for such a large network, we have to consider some small network that can improve the speed. Actually since the robot usally needs to classify within 10 people, it doesn't need a pretty strong model, but it needs efficiency.\n",
    " \n",
    "The work by Wu etc. [3] introdcued a lightened network A for face recognition, and the performance is still acceptable. So in this project, we chose to implement that network and use it in our final demo system. The network contains around 1,300K parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning is always not easy for face recognition, and choosing loss function is highly related to final face representation. Facenet [1] used the tripple loss, which tries to maximize the distance between different people but minimize the distance between the same person. But during the trainning process, you need to do hard-negative example mining in order to increase the performance. Thus it always take a long time and the convergence is poor.\n",
    "\n",
    "Parkhi [8] found out that actually instead of using triple loss at first, using classifier loss to pre-train the network will help the network to converge well. Wen [2] recently proposed one neat idea of using center loss. Actually the problem of using classifier loss is the variance is large within one class, since the classifier is just tring to find out a shape that can accuratly classify different classes. For the center loss, it added the variance penalty into the loss, so it forced the variance to be small for the same person. In our implementation we choosed to train a classifier and using the center loss. \n",
    "\n",
    "In order to train face recognition, we first need to crop out the thumbnail of the face. We first use the Zhang's [8] method to detect the face, this method tries to detect people's face through 3 small network and the performance is pertty good. In our final demo system, we used the dlib libarary since the speed is too slow for Zhang's [8] method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since it's not be able to include the large dataset here, I simply write down the steps to train the model\n",
    "# 1. Download face recognization dataset LFW (http://vis-www.cs.umass.edu/lfw/) as well as Casia(http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html)\n",
    "# 2. Preprocess LFW and Casia dataset and crop out people's face.\n",
    "#     \"python src/align/align_dataset_mtcnn.py ~/datasets/lfw/raw ~/remote/datasets/lfw/lfw_mtcnnpy_128 --image_size 128 --margin 24 --random_order\"\n",
    "#     \"python src/align/align_dataset_mtcnn.py ~/dataset/CASIA-WebFace ~/remote/datasets/lfw/casia_mtcnnpy_144 --image_size 144 --margin 24 --random_order\"\n",
    "# 3. Tranning Classifier: facenet_train_classifier.py\n",
    "#     python src/facenet_train_classifier.py --logs_base_dir ~/remote/logs/facenet/lightened --models_base_dir ~/remote/models/facenet/lightened/ --data_dir ~/remote/datasets/casia/casia_maxpy_mtcnnpy_144 --image_size 128 --model_def models.lightened --lfw_dir /home/xca64/remote/datasets/lfw/lfw_mtcnnpy_128 --optimizer RMSPROP --learning_rate 0.05 --max_nrof_epochs 80 --keep_probability 0.8 --random_crop --random_flip --learning_rate_schedule_file data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-5 --center_loss_factor 1e-4 --center_loss_alfa 0.9 --batch_size 45 --pretrained_model /home/xca64/remote/models/facenet/lightened/20170321-134041\n",
    "# 4. Lightened, Inception_V1 network: lightened.py, inception_resnet_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tennis1](../Content/Pics/Recognition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract People's Feature Using Trainned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the face recognition model, we can use the model to extract the feature for each person and then train a svm classifier. We put all the needed data inside the \"Content\" folder. In order to extract feature for new person, simply place new photos under the \"People\" folder and name the new folder with the person's name. The program will automatically crop the photo and use the trained neural network to extract the feature for all people  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solving PYTHONPATH Problem\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "current_folder_path, _ = os.path.split(os.getcwd())\n",
    "sys.path.insert(0, current_folder_path+'/src')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a long time, cause it will crop the feature at first and then extracte the features for each person. And make sure the PYTHONPATH is set to the src folder as previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Model directory: ../Content/NetworkModel/20170321-222346\n",
      "Loading network used to extract features\n",
      "Metagraph file: model-20170321-222346.meta\n",
      "Checkpoint file: model-20170321-222346.ckpt-80000\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0001.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0002.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0003.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0004.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0005.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0006.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0007.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0008.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0009.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0010.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0011.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0012.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0013.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0014.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0015.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0016.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0017.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0018.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0019.jpg\n",
      "../Content/People/Ariel_Sharon/Ariel_Sharon_0020.jpg\n",
      "../Content/People/Bicheng/0.jpg\n",
      "../Content/People/Bicheng/1.jpg\n",
      "../Content/People/Bicheng/10.jpg\n",
      "../Content/People/Bicheng/11.jpg\n",
      "../Content/People/Bicheng/12.jpg\n",
      "../Content/People/Bicheng/13.jpg\n",
      "../Content/People/Bicheng/14.jpg\n",
      "../Content/People/Bicheng/15.jpg\n",
      "../Content/People/Bicheng/16.jpg\n",
      "../Content/People/Bicheng/17.jpg\n",
      "../Content/People/Bicheng/18.jpg\n",
      "../Content/People/Bicheng/19.jpg\n",
      "../Content/People/Bicheng/2.jpg\n",
      "../Content/People/Bicheng/20.jpg\n",
      "../Content/People/Bicheng/21.jpg\n",
      "../Content/People/Bicheng/22.jpg\n",
      "../Content/People/Bicheng/23.jpg\n",
      "../Content/People/Bicheng/24.jpg\n",
      "../Content/People/Bicheng/25.jpg\n",
      "../Content/People/Bicheng/26.jpg\n",
      "../Content/People/Bicheng/27.jpg\n",
      "../Content/People/Bicheng/28.jpg\n",
      "../Content/People/Bicheng/29.jpg\n",
      "../Content/People/Bicheng/3.jpg\n",
      "../Content/People/Bicheng/30.jpg\n",
      "../Content/People/Bicheng/31.jpg\n",
      "../Content/People/Bicheng/32.jpg\n",
      "../Content/People/Bicheng/33.jpg\n",
      "../Content/People/Bicheng/34.jpg\n",
      "../Content/People/Bicheng/35.jpg\n",
      "../Content/People/Bicheng/36.jpg\n",
      "../Content/People/Bicheng/37.jpg\n",
      "../Content/People/Bicheng/38.jpg\n",
      "../Content/People/Bicheng/39.jpg\n",
      "../Content/People/Bicheng/4.jpg\n",
      "../Content/People/Bicheng/40.jpg\n",
      "../Content/People/Bicheng/41.jpg\n",
      "../Content/People/Bicheng/42.jpg\n",
      "../Content/People/Bicheng/43.jpg\n",
      "../Content/People/Bicheng/44.jpg\n",
      "Unable to align \"../Content/People/Bicheng/44.jpg\"\n",
      "../Content/People/Bicheng/45.jpg\n",
      "Unable to align \"../Content/People/Bicheng/45.jpg\"\n",
      "../Content/People/Bicheng/46.jpg\n",
      "Unable to align \"../Content/People/Bicheng/46.jpg\"\n",
      "../Content/People/Bicheng/47.jpg\n",
      "Unable to align \"../Content/People/Bicheng/47.jpg\"\n",
      "../Content/People/Bicheng/48.jpg\n",
      "Unable to align \"../Content/People/Bicheng/48.jpg\"\n",
      "../Content/People/Bicheng/49.jpg\n",
      "Unable to align \"../Content/People/Bicheng/49.jpg\"\n",
      "../Content/People/Bicheng/5.jpg\n",
      "../Content/People/Bicheng/50.jpg\n",
      "Unable to align \"../Content/People/Bicheng/50.jpg\"\n",
      "../Content/People/Bicheng/51.jpg\n",
      "Unable to align \"../Content/People/Bicheng/51.jpg\"\n",
      "../Content/People/Bicheng/52.jpg\n",
      "Unable to align \"../Content/People/Bicheng/52.jpg\"\n",
      "../Content/People/Bicheng/53.jpg\n",
      "Unable to align \"../Content/People/Bicheng/53.jpg\"\n",
      "../Content/People/Bicheng/54.jpg\n",
      "Unable to align \"../Content/People/Bicheng/54.jpg\"\n",
      "../Content/People/Bicheng/55.jpg\n",
      "Unable to align \"../Content/People/Bicheng/55.jpg\"\n",
      "../Content/People/Bicheng/56.jpg\n",
      "Unable to align \"../Content/People/Bicheng/56.jpg\"\n",
      "../Content/People/Bicheng/57.jpg\n",
      "../Content/People/Bicheng/58.jpg\n",
      "../Content/People/Bicheng/59.jpg\n",
      "../Content/People/Bicheng/6.jpg\n",
      "../Content/People/Bicheng/7.jpg\n",
      "../Content/People/Bicheng/8.jpg\n",
      "../Content/People/Bicheng/9.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0001.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0002.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0003.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0004.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0005.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0006.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0007.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0008.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0009.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0010.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0011.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0012.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0013.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0014.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0015.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0016.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0017.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0018.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0019.jpg\n",
      "../Content/People/Colin_Powell/Colin_Powell_0020.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0001.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0002.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0003.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0004.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0005.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0006.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0007.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0008.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0009.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0010.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0011.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0012.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0013.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0014.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0015.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0016.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0017.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0018.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0019.jpg\n",
      "../Content/People/Donald_Rumsfeld/Donald_Rumsfeld_0020.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0001.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0002.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0003.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0004.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0005.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0006.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0007.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0008.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0009.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0010.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0011.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0012.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0013.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0014.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0015.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0016.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0017.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0018.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0019.jpg\n",
      "../Content/People/Gerhard_Schroeder/Gerhard_Schroeder_0020.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0001.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0002.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0003.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0004.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0005.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0006.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0007.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0008.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0009.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0010.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0011.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0012.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0013.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0014.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0015.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0016.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0017.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0018.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0019.jpg\n",
      "../Content/People/Hugo_Chavez/Hugo_Chavez_0020.jpg\n",
      "../Content/People/Husha/0.jpg\n",
      "../Content/People/Husha/1.jpg\n",
      "../Content/People/Husha/10.jpg\n",
      "../Content/People/Husha/11.jpg\n",
      "../Content/People/Husha/12.jpg\n",
      "../Content/People/Husha/13.jpg\n",
      "../Content/People/Husha/14.jpg\n",
      "../Content/People/Husha/15.jpg\n",
      "../Content/People/Husha/16.jpg\n",
      "../Content/People/Husha/17.jpg\n",
      "../Content/People/Husha/18.jpg\n",
      "../Content/People/Husha/19.jpg\n",
      "../Content/People/Husha/2.jpg\n",
      "../Content/People/Husha/20.jpg\n",
      "../Content/People/Husha/21.jpg\n",
      "../Content/People/Husha/22.jpg\n",
      "Unable to align \"../Content/People/Husha/22.jpg\"\n",
      "../Content/People/Husha/23.jpg\n",
      "Unable to align \"../Content/People/Husha/23.jpg\"\n",
      "../Content/People/Husha/24.jpg\n",
      "Unable to align \"../Content/People/Husha/24.jpg\"\n",
      "../Content/People/Husha/25.jpg\n",
      "Unable to align \"../Content/People/Husha/25.jpg\"\n",
      "../Content/People/Husha/26.jpg\n",
      "Unable to align \"../Content/People/Husha/26.jpg\"\n",
      "../Content/People/Husha/27.jpg\n",
      "Unable to align \"../Content/People/Husha/27.jpg\"\n",
      "../Content/People/Husha/28.jpg\n",
      "Unable to align \"../Content/People/Husha/28.jpg\"\n",
      "../Content/People/Husha/29.jpg\n",
      "../Content/People/Husha/3.jpg\n",
      "../Content/People/Husha/30.jpg\n",
      "../Content/People/Husha/31.jpg\n",
      "../Content/People/Husha/32.jpg\n",
      "../Content/People/Husha/33.jpg\n",
      "../Content/People/Husha/34.jpg\n",
      "../Content/People/Husha/35.jpg\n",
      "../Content/People/Husha/36.jpg\n",
      "../Content/People/Husha/37.jpg\n",
      "../Content/People/Husha/38.jpg\n",
      "../Content/People/Husha/39.jpg\n",
      "../Content/People/Husha/4.jpg\n",
      "../Content/People/Husha/40.jpg\n",
      "../Content/People/Husha/41.jpg\n",
      "../Content/People/Husha/42.jpg\n",
      "../Content/People/Husha/43.jpg\n",
      "../Content/People/Husha/44.jpg\n",
      "../Content/People/Husha/45.jpg\n",
      "../Content/People/Husha/46.jpg\n",
      "../Content/People/Husha/47.jpg\n",
      "../Content/People/Husha/48.jpg\n",
      "../Content/People/Husha/49.jpg\n",
      "../Content/People/Husha/5.jpg\n",
      "../Content/People/Husha/50.jpg\n",
      "../Content/People/Husha/51.jpg\n",
      "../Content/People/Husha/52.jpg\n",
      "../Content/People/Husha/53.jpg\n",
      "../Content/People/Husha/54.jpg\n",
      "../Content/People/Husha/55.jpg\n",
      "../Content/People/Husha/56.jpg\n",
      "../Content/People/Husha/57.jpg\n",
      "../Content/People/Husha/58.jpg\n",
      "../Content/People/Husha/59.jpg\n",
      "../Content/People/Husha/6.jpg\n",
      "../Content/People/Husha/7.jpg\n",
      "../Content/People/Husha/8.jpg\n",
      "../Content/People/Husha/9.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0001.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0002.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0003.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0004.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0005.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0006.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0007.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0008.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0009.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0010.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0011.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0012.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0013.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0014.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0015.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0016.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0017.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0018.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0019.jpg\n",
      "../Content/People/Jean_Chretien/Jean_Chretien_0020.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0001.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0002.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0003.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0004.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0005.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0006.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0007.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0008.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0009.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0010.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0011.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0012.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0013.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0014.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0015.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0016.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0017.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0018.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0019.jpg\n",
      "../Content/People/John_Ashcroft/John_Ashcroft_0020.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0001.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0002.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0003.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0004.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0005.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0006.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0007.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0008.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0009.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0010.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0011.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0012.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0013.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0014.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0015.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0016.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0017.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0018.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0019.jpg\n",
      "../Content/People/Junichiro_Koizumi/Junichiro_Koizumi_0020.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0001.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0002.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0003.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0004.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0005.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0006.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0007.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0008.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0009.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0010.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0011.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0012.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0013.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0014.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0015.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0016.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0017.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0018.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0019.jpg\n",
      "../Content/People/Tony_Blair/Tony_Blair_0020.jpg\n",
      "../Content/People/Xiaochuan/0.jpg\n",
      "../Content/People/Xiaochuan/1.jpg\n",
      "../Content/People/Xiaochuan/10.jpg\n",
      "../Content/People/Xiaochuan/11.jpg\n",
      "../Content/People/Xiaochuan/12.jpg\n",
      "../Content/People/Xiaochuan/13.jpg\n",
      "../Content/People/Xiaochuan/14.jpg\n",
      "../Content/People/Xiaochuan/15.jpg\n",
      "../Content/People/Xiaochuan/16.jpg\n",
      "../Content/People/Xiaochuan/17.jpg\n",
      "../Content/People/Xiaochuan/18.jpg\n",
      "../Content/People/Xiaochuan/19.jpg\n",
      "../Content/People/Xiaochuan/2.jpg\n",
      "../Content/People/Xiaochuan/20.jpg\n",
      "../Content/People/Xiaochuan/21.jpg\n",
      "../Content/People/Xiaochuan/22.jpg\n",
      "../Content/People/Xiaochuan/23.jpg\n",
      "../Content/People/Xiaochuan/24.jpg\n",
      "../Content/People/Xiaochuan/25.jpg\n",
      "../Content/People/Xiaochuan/26.jpg\n",
      "../Content/People/Xiaochuan/27.jpg\n",
      "../Content/People/Xiaochuan/28.jpg\n",
      "../Content/People/Xiaochuan/29.jpg\n",
      "../Content/People/Xiaochuan/3.jpg\n",
      "../Content/People/Xiaochuan/30.jpg\n",
      "../Content/People/Xiaochuan/31.jpg\n",
      "../Content/People/Xiaochuan/32.jpg\n",
      "../Content/People/Xiaochuan/33.jpg\n",
      "../Content/People/Xiaochuan/34.jpg\n",
      "../Content/People/Xiaochuan/35.jpg\n",
      "../Content/People/Xiaochuan/36.jpg\n",
      "../Content/People/Xiaochuan/37.jpg\n",
      "../Content/People/Xiaochuan/38.jpg\n",
      "../Content/People/Xiaochuan/39.jpg\n",
      "../Content/People/Xiaochuan/4.jpg\n",
      "../Content/People/Xiaochuan/40.jpg\n",
      "../Content/People/Xiaochuan/41.jpg\n",
      "../Content/People/Xiaochuan/42.jpg\n",
      "../Content/People/Xiaochuan/43.jpg\n",
      "../Content/People/Xiaochuan/44.jpg\n",
      "../Content/People/Xiaochuan/45.jpg\n",
      "../Content/People/Xiaochuan/46.jpg\n",
      "../Content/People/Xiaochuan/47.jpg\n",
      "../Content/People/Xiaochuan/48.jpg\n",
      "../Content/People/Xiaochuan/49.jpg\n",
      "../Content/People/Xiaochuan/5.jpg\n",
      "../Content/People/Xiaochuan/50.jpg\n",
      "../Content/People/Xiaochuan/51.jpg\n",
      "../Content/People/Xiaochuan/52.jpg\n",
      "../Content/People/Xiaochuan/53.jpg\n",
      "../Content/People/Xiaochuan/54.jpg\n",
      "../Content/People/Xiaochuan/55.jpg\n",
      "../Content/People/Xiaochuan/56.jpg\n",
      "../Content/People/Xiaochuan/57.jpg\n",
      "../Content/People/Xiaochuan/58.jpg\n",
      "../Content/People/Xiaochuan/59.jpg\n",
      "../Content/People/Xiaochuan/6.jpg\n",
      "../Content/People/Xiaochuan/7.jpg\n",
      "../Content/People/Xiaochuan/8.jpg\n",
      "../Content/People/Xiaochuan/9.jpg\n",
      "Total number of images: 360\n",
      "Number of successfully aligned images: 0\n",
      "\n",
      "\n",
      "Starting to extract features\n",
      "Total number of images: 340\n",
      "Runnning forward pass on images\n",
      "Processing Patch 0/7\n",
      "Processing Patch 1/7\n",
      "Processing Patch 2/7\n",
      "Processing Patch 3/7\n",
      "Processing Patch 4/7\n",
      "Processing Patch 5/7\n",
      "Processing Patch 6/7\n"
     ]
    }
   ],
   "source": [
    "%run ../src/align/train_person_recognition_with_dlib.py --input_dir ../Content/People --output_dir ../Content/People_Cropped --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --feature_dir ../Content/PeopleFeature --feature_name emb_array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('People Feature Example ', array([ -9.65369120e-02,  -7.23371133e-02,  -6.66213483e-02,\n",
      "        -6.02303855e-02,   6.86687008e-02,  -1.32398885e-02,\n",
      "         1.81062832e-01,   5.12475595e-02,  -1.12503646e-02,\n",
      "        -1.03096440e-02,  -1.01644710e-01,   3.19180302e-02,\n",
      "         6.83150515e-02,  -2.11343262e-02,  -3.90157327e-02,\n",
      "        -6.26420900e-02,   4.89030592e-02,   3.60059887e-02,\n",
      "        -2.01329961e-02,  -4.18936685e-02,   1.58133328e-01,\n",
      "         2.38300741e-01,   7.51493797e-02,  -3.29715083e-03,\n",
      "        -1.31749779e-01,  -1.23721352e-02,  -2.29130965e-02,\n",
      "         2.30019018e-02,  -4.91008908e-02,  -1.70702323e-01,\n",
      "         7.01758042e-02,   1.36246413e-01,  -3.03391665e-02,\n",
      "         1.27486726e-02,   4.71237786e-02,  -1.69819549e-01,\n",
      "         1.17722861e-01,   4.47117761e-02,   1.34126581e-02,\n",
      "         9.09058899e-02,  -3.85622345e-02,   4.87625077e-02,\n",
      "        -1.53344953e-02,  -6.48706779e-02,  -9.42303836e-02,\n",
      "         3.52343395e-02,   1.28600404e-01,   1.61406294e-01,\n",
      "         2.47464725e-03,   7.70620443e-03,   8.35519060e-02,\n",
      "         7.95460120e-02,   1.31115252e-02,   1.79143623e-01,\n",
      "        -1.43923258e-04,  -7.34328628e-02,  -4.62975390e-02,\n",
      "         8.96540433e-02,   3.03995498e-02,   6.11588657e-02,\n",
      "        -7.15649202e-02,  -5.83037473e-02,   9.41241160e-03,\n",
      "         1.80735569e-02,   1.29086059e-03,  -9.27299932e-02,\n",
      "        -4.42483947e-02,   9.05455276e-02,  -5.15709259e-03,\n",
      "         1.41595855e-01,   2.11684987e-01,  -1.03738382e-01,\n",
      "        -5.49441390e-02,  -5.10532819e-02,  -7.03018680e-02,\n",
      "         8.97708461e-02,  -3.67934592e-02,   1.21532880e-01,\n",
      "        -2.88470164e-02,  -1.19930401e-01,   3.26875560e-02,\n",
      "         1.62556767e-02,  -7.63354003e-02,   3.79930884e-02,\n",
      "        -1.58884704e-01,   1.12811178e-01,   1.38473749e-01,\n",
      "        -1.07222222e-01,   1.31868552e-02,   2.18361095e-02,\n",
      "         1.55684069e-01,   5.57785742e-02,  -2.04449948e-02,\n",
      "        -3.03611010e-02,  -8.92002732e-02,   3.35176811e-02,\n",
      "         1.62082911e-02,   1.62254736e-01,  -1.87912770e-02,\n",
      "        -6.50885776e-02,   6.18944727e-02,  -4.54158299e-02,\n",
      "         4.87827063e-02,   7.65258400e-03,   7.22279996e-02,\n",
      "        -8.84011835e-02,  -1.00490972e-01,  -9.58720520e-02,\n",
      "         3.68380314e-03,   1.96915656e-01,   1.63194627e-01,\n",
      "         1.86699599e-01,  -1.51735485e-01,  -1.60551563e-01,\n",
      "         8.81606638e-02,  -3.86228599e-02,   1.69212408e-02,\n",
      "         1.18454501e-01,   9.61619318e-02,   9.80838984e-02,\n",
      "         3.36045884e-02,  -6.08108705e-03,  -1.58619042e-02,\n",
      "        -6.06891662e-02,  -3.75671010e-03,   1.25174895e-01,\n",
      "         3.47967520e-02,   1.01821646e-01]))\n",
      "('People Label', 'Ariel_Sharon')\n"
     ]
    }
   ],
   "source": [
    "# The extracted feature are stored in the PeopleFeature dir, and you can have a look of the feature by\n",
    "# uncommenting followed code\n",
    "\n",
    "npfile=np.load('../Content/PeopleFeature/emb_array.npz')\n",
    "print('People Feature Example ', npfile['emb_array'][0])\n",
    "print('People Label', npfile['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, at first we tested the system's accuracy to ten-fold cross validation by 25 times\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1 --test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning and Storing SVM model\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, and store the trainned SVM model\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Model directory: ../Content/NetworkModel/20170321-222346\n",
      "Loading network used to extract features\n",
      "Metagraph file: model-20170321-222346.meta\n",
      "Checkpoint file: model-20170321-222346.ckpt-80000\n",
      "../Content/Test/photos/Xiaochuan/0.jpg\n",
      "../Content/Test/photos/Xiaochuan/1.jpg\n",
      "../Content/Test/photos/Xiaochuan/2.jpg\n",
      "../Content/Test/photos/Xiaochuan/3.jpg\n",
      "../Content/Test/photos/Xiaochuan/4.jpg\n",
      "../Content/Test/photos/Xiaochuan/5.jpg\n",
      "../Content/Test/photos/Xiaochuan/6.jpg\n",
      "../Content/Test/photos/Xiaochuan/7.jpg\n",
      "Total number of images: 8\n",
      "Number of successfully aligned images: 8\n",
      "\n",
      "\n",
      "Starting to extract features\n",
      "Crop pics spend 0.887 seconds\n",
      "Extract feature spend 2.409 seconds\n",
      "Classifier spend 0.002 seconds\n",
      "Predicted Persons\n",
      "['Xiaochuan' 'Xiaochuan' 'Xiaochuan' 'Xiaochuan' 'Xiaochuan' 'Xiaochuan'\n",
      " 'Xiaochuan' 'Xiaochuan']\n"
     ]
    }
   ],
   "source": [
    "# The test photo are located in the test folder and the predicted label should be Xiaochuan\n",
    "# This step may take a long time, cause it will load the tensorflow model\n",
    "# It may crash the notebook, seems like it's the dlib libraries' problem, but you can copy the commands to terminal and run it\n",
    "%run ../src/align/recognize_person_with_dlib.py --input_dir ../Content/Test/photos --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Face Detection and Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tennis1](../Content/Pics/Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect the people in real-time, we don't choose the model proposed in [4]. In our experiment, it will take around 1 second which is not acceptable. We instead used the dlib face detection method which will take around 50ms, and we place it to a separate process for detection. Once it detected a new person in the video, it will invoke the face recognition part to classify this new person. In the main loop, we used dlib tracking method and it can achieve real-time tracking. \n",
    "\n",
    "In all, we can achieve real-time face recognition for multiple people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note, it may complain the dlib doesn't have imshow function, this related to the dlib problem, Mac may have \n",
    "# this problem, but ubuntu doesn't  ['module' object has no attribute 'image_window']\n",
    "%run ../src/align/detect_tracking_recognize.py --input_video ../Content/Test/Two.mov --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../src/align/detect_tracking_recognize.py --input_video ../Content/Test/Three.mov --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tennis1](../Content/Pics/tracking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the whole process, we experienced with designing neural network and placing it to real usage. Currently, our demo are limited to the desktop, since the migration to embedded device faces the memory problem (we only have 1g memory on the chip board so far), and tensorflow failed to load on that device. \n",
    "\n",
    "We want to have all process on board since it can respond in a very short time. In the future, we may use Hinton's method [9] to distill accurate models into small models for the robot usage. And trainning multiple-task network also seems to be a proper way to run deep learning on robot, because it can process multiple tasks using only one forward run.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Florian Schroff, Dmitry Kalenichenko, James Philbin; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 815-823\n",
    "\n",
    "[2] Wen, Yandong, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. \"A discriminative feature learning approach for deep face recognition.\" In European Conference on Computer Vision, pp. 499-515. Springer International Publishing, 2016.\n",
    "\n",
    "[3] Wu, Xiang, Ran He, and Zhenan Sun. \"A lightened cnn for deep face representation.\" In 2015 IEEE Conference on IEEE Computer Vision and Pattern Recognition (CVPR). 2015.\n",
    "\n",
    "[4] Zhang, Kaipeng, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. \"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\" IEEE Signal Processing Letters 23, no. 10 (2016): 1499-1503.\n",
    "\n",
    "[5] https://github.com/davidsandberg/facenet\n",
    "\n",
    "[6] https://cmusatyalab.github.io/openface/\n",
    "\n",
    "[7] Dong Yi, Zhen Lei, Shengcai Liao and Stan Z. Li, “Learning Face Representation from Scratch”. arXiv preprint[] arXiv:1411.7923. 2014\n",
    "\n",
    "[8] Parkhi, Omkar M., Andrea Vedaldi, and Andrew Zisserman. \"Deep Face Recognition.\" In BMVC, vol. 1, no. 3, p. 6. 2015.\n",
    "\n",
    "[9] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
