{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final goal is to build intelligent robot that can play with people. We want to use the cutting-edge deep learning\n",
    "that help the robot to see, understand and react. Even though many deep learning methods are open sourced online, we need\n",
    "to build an integrated system. And we need to find out the balance between accuracy and speed, since everyone wants an agile \n",
    "robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our idea of applying deep learning methods to robot, in this assignment we choose to apply face recognition onto robot. Another consideration is in order to let the robot inteact with people, the first step is to help the robot recognize people. Even though face recognition is an active research topic and there are many open-sourced project for face recognition, for our own robot we want it to be accurate and fast. It's not just simply calling some api, we want to integate it into our whole system and know how to optimize it.\n",
    "\n",
    "We start from tranning our model for face recognition from a dataset containing 10,575 people and 494,414 images. Different networks architectures have been tested. And finally, we integrated the detecting, tracking and recognizing for real-time performance.\n",
    "\n",
    "We build the whole project based on David's [5] tensorflow implementation of facenet [1], and referenced from the CMU project OpenFace[6] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the results for the whole system:\n",
    "    1. Face recognization accuracy: \n",
    "        a. Google Inception Net V1: 99% on LFW datset\n",
    "        b. Lightened network: 93% on LFW dataset\n",
    "    2. Face classification accuracy: ~100% with a datsets with 10 people\n",
    "    2. Face recognization time usage on cpu: 50~100ms/One person, 100~150ms/Two person, 150~200ms/Three person\n",
    "    3. Face recognization time usage on gpu: roughly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Face Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from using David's[5] pre-trained model using Inception-V1, trainned on Casia dataset [7]. This model has ~99% accuracy on LFW dataset, which means it's already good enough for us to use!! But considering the time efficiency and memory usage for such a large network, we have to consider some small network that can improve the speed. Actually since the robot usally need to classify within 10 people, it doesn't need a pretty strong model, but it needs efficiency.\n",
    " \n",
    "The work by Wu etc. [3] introdcued a lightened network for face recognition, and the performance is still acceptable. So in this project, we chose to implement that network and use it in our final demo system. The network architecture is as following.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning is always not easy for face recognition, and choosing loss function is highly related to final face representation. Facenet [1] used the tripple loss, which tries to maximize the distance between different people but minimize the distance between the same person. But during the trainning process, you need to do hard-negative example mining in order to increase the performance. Thus it always take a long time and the convergence is poor.\n",
    "\n",
    "Parkhi [8] found out that actually in stead of using triple loss at first, using classifier loss to pre-train the network will help the network to converge well. Wen [2] recently proposed one neat idea of using center loss. Actually the problem of using classifier loss is the variation if large within one class, since the classifier is just tring to find out a shape that can accuratly classify different classes. For the center loss, it added the variational penalty into the loss, so it forced the variance to be small for the same person. In our implementation we choosed to train classifier and using the center loss. \n",
    "\n",
    "In order to train face recognition, we first need to crop out the thumbnail of the face. We first use the Zhang's [8] method to detect the face, this method tries to detect people's face through 3 small network and the performance is pertty good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's be able to include the large dataset here, I simply write down the files that we used to train the model\n",
    "1. Cropping the thumbnail: align_dataset_mtcnn.py\n",
    "2. Tranning Classifier: facenet_train_classifier.py\n",
    "3. Lightened, Inception_V1 network: lightened.py, inception_resnet_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract People's Feature Using Trainned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the face recognition model, we can use the model to extract the feature for each person and then train a svm classifier. We put all the needed data inside the \"Content\" folder. In order to extract feature for new person, simply place new photos under the \"People\" folder and name the new folder with the person's name. The program will automatically crop the photo and use the trained neural network to extract the feature for all people  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solving PYTHONPATH Problem\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '/home/xca64/remote/GitHub/facenet/src')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a long time, cause it will crop the feature at first and then extracte the features for each person. And make sure the PYTHONPATH is set to the src folder as previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../src/align/train_person_recognition_with_dlib.py --input_dir ../Content/People --output_dir ../Content/People_Cropped --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --feature_dir ../Content/PeopleFeature --feature_name emb_array \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The extracted feature are stored in the PeopleFeature dir, and you can have a look of the feature by\n",
    "# uncommenting followed code\n",
    "\n",
    "# npfile=np.load('../Content/PeopleFeature/emb_array.npz')\n",
    "# print('People Feature Example ', npfile['emb_array'][0])\n",
    "# print('People Label', npfile['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.972222222222\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.972222222222\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, at first we tested the system's accuracy to ten-fold cross validation by 25 times\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1 --test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning and Storing SVM model\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Running the SVM Training, and store the trainned SVM model\n",
    "%run ../util/train_svm.py --npz_file_dir ../Content/PeopleFeature/emb_array.npz --C 4 --svm_model_dir ../Content/SVMModel --gamma 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Model directory: ../Content/NetworkModel/20170321-222346\n",
      "Loading network used to extract features\n",
      "Metagraph file: model-20170321-222346.meta\n",
      "Checkpoint file: model-20170321-222346.ckpt-80000\n",
      "../Content/Test/photos/Xiaochuan/0.jpg\n",
      "../Content/Test/photos/Xiaochuan/1.jpg\n",
      "../Content/Test/photos/Xiaochuan/2.jpg\n",
      "../Content/Test/photos/Xiaochuan/3.jpg\n",
      "../Content/Test/photos/Xiaochuan/4.jpg\n",
      "../Content/Test/photos/Xiaochuan/5.jpg\n",
      "../Content/Test/photos/Xiaochuan/6.jpg\n",
      "../Content/Test/photos/Xiaochuan/7.jpg\n",
      "Total number of images: 8\n",
      "Number of successfully aligned images: 8\n",
      "\n",
      "\n",
      "Starting to extract features\n",
      "Crop pics spend 0.461 seconds\n"
     ]
    }
   ],
   "source": [
    "# The test photo are located in the test folder and the predicted label should be Xiaochuan\n",
    "# This step may take a long time, cause it will load the tensorflow model\n",
    "# It may crash the notebook, seems like it's the dlib libraries' problem, but you can copy the commands to terminal and run it\n",
    "%run ../src/align/recognize_person_with_dlib.py --input_dir ../Content/Test/photos --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Face Detection and Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect the people in real-time, we don't choose the model proposed in [4]. In our experiment, it will take around 1 second which is not acceptable. We instead used the dlib face detection method which will take around 50ms, and we place it to a separate process for detection. Once it detected a new person in the video, it will invoke the face recognition part to classify this new person. In the main loop, we used dlib tracking method and it can achieve real-time tracking. \n",
    "\n",
    "In all, we can achieve real-time face recognition for multiple people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../src/align/detect_tracking_recognize.py --input_video ../Content/Test/Two.mov --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Model directory: ../Content/NetworkModel/20170321-222346\n",
      "Loading network used to extract features\n",
      "Metagraph file: model-20170321-222346.meta\n",
      "Checkpoint file: model-20170321-222346.ckpt-80000\n",
      "Processing Frame 40\n",
      "Processing Frame 41\n",
      "Processing Frame 42\n",
      "Processing Frame 43\n",
      "Processing Frame 44\n",
      "Processing Frame 45\n",
      "Processing Frame 46\n",
      "Processing Frame 47\n",
      "Processing Frame 48\n",
      "Processing Frame 49\n",
      "Processing Frame 50\n",
      "Processing Frame 51\n",
      "Processing Frame 52\n",
      "Processing Frame 53\n",
      "Processing Frame 54\n",
      "Processing Frame 55\n",
      "Processing Frame 56\n",
      "Processing Frame 57\n",
      "Processing Frame 58\n",
      "Processing Frame 59\n",
      "Processing Frame 60\n",
      "(1, 128, 128, 3)\n",
      "Predicted Persons, spending time  0.165466070175\n",
      "['Bicheng']\n",
      "Processing Frame 61\n",
      "Processing Frame 62\n",
      "Processing Frame 63\n",
      "Processing Frame 64\n",
      "Processing Frame 65\n",
      "Processing Frame 66\n",
      "Processing Frame 67\n",
      "Processing Frame 68\n",
      "Processing Frame 69\n",
      "Processing Frame 70\n",
      "Processing Frame 71\n",
      "Processing Frame 72\n",
      "Processing Frame 73\n",
      "Processing Frame 74\n",
      "Processing Frame 75\n",
      "Processing Frame 76\n",
      "Processing Frame 77\n",
      "Processing Frame 78\n",
      "Processing Frame 79\n",
      "Processing Frame 80\n",
      "Processing Frame 81\n",
      "Processing Frame 82\n",
      "Processing Frame 83\n",
      "Processing Frame 84\n",
      "Processing Frame 85\n",
      "Processing Frame 86\n",
      "Processing Frame 87\n",
      "Processing Frame 88\n",
      "Processing Frame 89\n",
      "Processing Frame 90\n",
      "Processing Frame 91\n",
      "Processing Frame 92\n",
      "Processing Frame 93\n",
      "Processing Frame 94\n",
      "Processing Frame 95\n",
      "Processing Frame 96\n",
      "Processing Frame 97\n",
      "Processing Frame 98\n",
      "Processing Frame 99\n",
      "Processing Frame 100\n",
      "Processing Frame 101\n",
      "Processing Frame 102\n",
      "Processing Frame 103\n",
      "Processing Frame 104\n",
      "Processing Frame 105\n",
      "Processing Frame 106\n",
      "Processing Frame 107\n",
      "Processing Frame 108\n",
      "Processing Frame 109\n",
      "Processing Frame 110\n",
      "Processing Frame 111\n",
      "Processing Frame 112\n",
      "Processing Frame 113\n",
      "Processing Frame 114\n",
      "Processing Frame 115\n",
      "Processing Frame 116\n",
      "Processing Frame 117\n",
      "Processing Frame 118\n",
      "Processing Frame 119\n",
      "Processing Frame 120\n",
      "Processing Frame 121\n",
      "Processing Frame 122\n",
      "Processing Frame 123\n",
      "Processing Frame 124\n",
      "Processing Frame 125\n",
      "Processing Frame 126\n",
      "Processing Frame 127\n",
      "Processing Frame 128\n",
      "Processing Frame 129\n",
      "Processing Frame 130\n",
      "Processing Frame 131\n",
      "Processing Frame 132\n",
      "Processing Frame 133\n",
      "Processing Frame 134\n",
      "Processing Frame 135\n",
      "Processing Frame 136\n",
      "Processing Frame 137\n",
      "Processing Frame 138\n",
      "Processing Frame 139\n",
      "Processing Frame 140\n",
      "Processing Frame 141\n",
      "Processing Frame 142\n",
      "Processing Frame 143\n",
      "Processing Frame 144\n",
      "Processing Frame 145\n",
      "Processing Frame 146\n",
      "Processing Frame 147\n",
      "Processing Frame 148\n",
      "Processing Frame 149\n",
      "Processing Frame 150\n",
      "Processing Frame 151\n",
      "Processing Frame 152\n",
      "Processing Frame 153\n",
      "Processing Frame 154\n",
      "Processing Frame 155\n",
      "Processing Frame 156\n",
      "Processing Frame 157\n",
      "Processing Frame 158\n",
      "Processing Frame 159\n",
      "Processing Frame 160\n",
      "(2, 128, 128, 3)\n",
      "Predicted Persons, spending time  0.00739502906799\n",
      "['Xiaochuan' 'Bicheng']\n",
      "Processing Frame 161\n",
      "Processing Frame 162\n",
      "Processing Frame 163\n",
      "Processing Frame 164\n",
      "Processing Frame 165\n",
      "Processing Frame 166\n",
      "Processing Frame 167\n",
      "Processing Frame 168\n",
      "Processing Frame 169\n",
      "Processing Frame 170\n",
      "Processing Frame 171\n",
      "Processing Frame 172\n",
      "Processing Frame 173\n",
      "Processing Frame 174\n",
      "Processing Frame 175\n",
      "Processing Frame 176\n",
      "Processing Frame 177\n",
      "Processing Frame 178\n",
      "Processing Frame 179\n",
      "Processing Frame 180\n",
      "Processing Frame 181\n",
      "Processing Frame 182\n",
      "Processing Frame 183\n",
      "Processing Frame 184\n",
      "Processing Frame 185\n",
      "Processing Frame 186\n",
      "Processing Frame 187\n",
      "Processing Frame 188\n",
      "Processing Frame 189\n",
      "Processing Frame 190\n",
      "Processing Frame 191\n",
      "Processing Frame 192\n",
      "Processing Frame 193\n",
      "Processing Frame 194\n",
      "Processing Frame 195\n",
      "Processing Frame 196\n",
      "Processing Frame 197\n",
      "Processing Frame 198\n",
      "Processing Frame 199\n",
      "Processing Frame 200\n",
      "Processing Frame 201\n",
      "Processing Frame 202\n",
      "Processing Frame 203\n",
      "Processing Frame 204\n",
      "Processing Frame 205\n",
      "Processing Frame 206\n",
      "Processing Frame 207\n",
      "Processing Frame 208\n",
      "Processing Frame 209\n",
      "Processing Frame 210\n",
      "Processing Frame 211\n",
      "Processing Frame 212\n",
      "Processing Frame 213\n",
      "Processing Frame 214\n",
      "Processing Frame 215\n",
      "Processing Frame 216\n",
      "Processing Frame 217\n",
      "Processing Frame 218\n",
      "Processing Frame 219\n",
      "Processing Frame 220\n",
      "Processing Frame 221\n",
      "Processing Frame 222\n",
      "Processing Frame 223\n",
      "Processing Frame 224\n",
      "Processing Frame 225\n",
      "Processing Frame 226\n",
      "Processing Frame 227\n",
      "Processing Frame 228\n",
      "Processing Frame 229\n",
      "Processing Frame 230\n",
      "Processing Frame 231\n",
      "Processing Frame 232\n",
      "Processing Frame 233\n",
      "Processing Frame 234\n",
      "Processing Frame 235\n",
      "Processing Frame 236\n",
      "Processing Frame 237\n",
      "Processing Frame 238\n",
      "Processing Frame 239\n",
      "Processing Frame 240\n",
      "Processing Frame 241\n",
      "Processing Frame 242\n",
      "Processing Frame 243\n",
      "Processing Frame 244\n",
      "Processing Frame 245\n",
      "Processing Frame 246\n",
      "Processing Frame 247\n",
      "Processing Frame 248\n",
      "Processing Frame 249\n",
      "Processing Frame 250\n",
      "Processing Frame 251\n",
      "Processing Frame 252\n",
      "Processing Frame 253\n",
      "Processing Frame 254\n",
      "Processing Frame 255\n",
      "Processing Frame 256\n",
      "Processing Frame 257\n",
      "Processing Frame 258\n",
      "Processing Frame 259\n",
      "Processing Frame 260\n",
      "Processing Frame 261\n",
      "Processing Frame 262\n",
      "Processing Frame 263\n",
      "Processing Frame 264\n",
      "Processing Frame 265\n",
      "Processing Frame 266\n",
      "Processing Frame 267\n",
      "Processing Frame 268\n",
      "Processing Frame 269\n",
      "Processing Frame 270\n",
      "Processing Frame 271\n",
      "Processing Frame 272\n",
      "Processing Frame 273\n",
      "Processing Frame 274\n",
      "Processing Frame 275\n",
      "Processing Frame 276\n",
      "Processing Frame 277\n",
      "Processing Frame 278\n",
      "Processing Frame 279\n",
      "Processing Frame 280\n",
      "(3, 128, 128, 3)\n",
      "Predicted Persons, spending time  0.00958108901978\n",
      "['Xiaochuan' 'Bicheng' 'Husha']\n",
      "Processing Frame 281\n",
      "Processing Frame 282\n",
      "Processing Frame 283\n",
      "Processing Frame 284\n",
      "Processing Frame 285\n",
      "Processing Frame 286\n",
      "Processing Frame 287\n",
      "Processing Frame 288\n",
      "Processing Frame 289\n",
      "Processing Frame 290\n",
      "Processing Frame 291\n",
      "Processing Frame 292\n",
      "Processing Frame 293\n",
      "Processing Frame 294\n",
      "Processing Frame 295\n",
      "Processing Frame 296\n",
      "Processing Frame 297\n",
      "Processing Frame 298\n",
      "Processing Frame 299\n",
      "Processing Frame 300\n",
      "Processing Frame 301\n",
      "Processing Frame 302\n",
      "Processing Frame 303\n",
      "Processing Frame 304\n",
      "Processing Frame 305\n",
      "Processing Frame 306\n",
      "Processing Frame 307\n",
      "Processing Frame 308\n",
      "Processing Frame 309\n",
      "Processing Frame 310\n",
      "Processing Frame 311\n",
      "Processing Frame 312\n",
      "Processing Frame 313\n",
      "Processing Frame 314\n",
      "Processing Frame 315\n",
      "Processing Frame 316\n",
      "Processing Frame 317\n",
      "Processing Frame 318\n",
      "Processing Frame 319\n",
      "Processing Frame 320\n",
      "Processing Frame 321\n",
      "Processing Frame 322\n",
      "Processing Frame 323\n",
      "Processing Frame 324\n",
      "Processing Frame 325\n",
      "Processing Frame 326\n",
      "Processing Frame 327\n",
      "Processing Frame 328\n",
      "Processing Frame 329\n",
      "Processing Frame 330\n",
      "Processing Frame 331\n",
      "Processing Frame 332\n",
      "Processing Frame 333\n",
      "Processing Frame 334\n",
      "Processing Frame 335\n",
      "Processing Frame 336\n",
      "Processing Frame 337\n",
      "Processing Frame 338\n",
      "Processing Frame 339\n",
      "Processing Frame 340\n",
      "Processing Frame 341\n",
      "Processing Frame 342\n",
      "Processing Frame 343\n",
      "Processing Frame 344\n",
      "Processing Frame 345\n",
      "Processing Frame 346\n",
      "Processing Frame 347\n",
      "Processing Frame 348\n",
      "Processing Frame 349\n",
      "Processing Frame 350\n",
      "Processing Frame 351\n",
      "Processing Frame 352\n",
      "Processing Frame 353\n",
      "Processing Frame 354\n",
      "Processing Frame 355\n",
      "Processing Frame 356\n",
      "Processing Frame 357\n",
      "Processing Frame 358\n",
      "Processing Frame 359\n",
      "Processing Frame 360\n",
      "Processing Frame 361\n",
      "Processing Frame 362\n",
      "Processing Frame 363\n",
      "Processing Frame 364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/cs/vml2/xca64/GitHub/facenet/src/align/detect_tracking_recognize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/vml2/xca64/GitHub/facenet/src/align/detect_tracking_recognize.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtracker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrackers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run ../src/align/detect_tracking_recognize.py --input_video ../Content/Test/Three.mov --image_size 128 --margin 24 --model_dir ../Content/NetworkModel/20170321-222346 --svm_model_dir ../Content/SVMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the whole process, we experienced with designing neural network and placing it to real usage. Currently, our demo are limited to the desktop, since the migration to embedded device faces the memory problem (we only have 1g memory on the chip board we have so far), and tensorflow failed to load on that device. \n",
    "\n",
    "We want to have all process on board since it can respond in a very short time. In the future, we may use Hinton's method [9] to distill accurate models into small models for the robot usage. And trainning multiple-task network also seems to be a proper way to run deep learning on robot.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Florian Schroff, Dmitry Kalenichenko, James Philbin; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 815-823\n",
    "\n",
    "[2] Wen, Yandong, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. \"A discriminative feature learning approach for deep face recognition.\" In European Conference on Computer Vision, pp. 499-515. Springer International Publishing, 2016.\n",
    "\n",
    "[3] Wu, Xiang, Ran He, and Zhenan Sun. \"A lightened cnn for deep face representation.\" In 2015 IEEE Conference on IEEE Computer Vision and Pattern Recognition (CVPR). 2015.\n",
    "\n",
    "[4] Zhang, Kaipeng, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. \"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\" IEEE Signal Processing Letters 23, no. 10 (2016): 1499-1503.\n",
    "\n",
    "[5] https://github.com/davidsandberg/facenet\n",
    "\n",
    "[6] https://cmusatyalab.github.io/openface/\n",
    "\n",
    "[7] Dong Yi, Zhen Lei, Shengcai Liao and Stan Z. Li, “Learning Face Representation from Scratch”. arXiv preprint[] arXiv:1411.7923. 2014\n",
    "\n",
    "[8] Parkhi, Omkar M., Andrea Vedaldi, and Andrew Zisserman. \"Deep Face Recognition.\" In BMVC, vol. 1, no. 3, p. 6. 2015.\n",
    "\n",
    "[9] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
